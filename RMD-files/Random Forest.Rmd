---
title: "Random Forest"
author: "Tyki Wada"
date: "1/29/2018"
output: html_document
---


Random Forest
Jan 29

How fast can I go before I end up getting a ticket in Montgomery County, MD?


Finding quality data is challenging, let alone discovering big data with relevant details is even more difficult. There are useful open source APIs that people have made available to the public, yet at times, it is difficult to retrieve pure data responses. So, unfortunately this dataset was just handed to me. Usually, I spend about a good 20-30 minutes combing through the internet in search for new datasets to work on with new testing. I wish there was a repository of some sorts that had everything I wanted.


To figure out how fast you can go in Maryland before you get ticketed, we will use a Random Forest technique. Random Forest is a tree model using bootstrapped trees. It is probably known as one of the most efficient, accurate algorithms that can handle thousands of input variables. It estimates unbiased importance regardless of missing data and is an amazing method to compute interactions between clusters of data.



Also known as a Random Decision Forest, this model takes an N number of classification trees and bootstraps (sampling with replacement) the trees. Each tree is counted as a "vote". The forest then chooses the classification with the most votes. For visual learners, the image below may help with a general outline of how this works.



This model utilizes a comparison of a test dataset with a training dataset to measure accuracy. This idea of training and test datasets are heavily used in machine-learning algorithms (Random Forests may be used in Machine Learning). Simply stated, training data pairs the input with an expected output to make sure that an obvious answer is returned correct and estimates the accuracy of the selected approach with a test dataset (How well your model has been trained depends on the size of your data, the value you would like to predict, input etc). After we test it, you have a specific model for this situation! A little sacrifice of prediction for information due to a large dependence on data variables, but a powerful tool nonetheless.


I know that was a mouthful, but it is easier to perform than it seems. Let's give it a try.


Montgomery County has a constantly updating dataset of traffic violation information. download the info here. 

This dataset usually takes a while depending on your hard-drive. Mine took about 20 minutes. Below, we take a glimpse of the data and see how there are a total of 1,244,404 rows and 35 variables.
```{r}
library(dplyr)

stops.full <- read.csv("http://data.montgomerycountymd.gov/api/views/4mse-ku6q/rows.csv?accessType=DOWNLOAD",header=TRUE,as.is=TRUE)

glimpse(stops.full)

```

Next, we'll have to filter the dataset to include all the data up to the last calendar year and the variables: Sub Agency, Accident, Seat-Belt, Personal Injury, Property Damage, Commercial License, Alcohol, Work zone, Color of car, Whether the driver contributed to an accident, Race, Gender, Auto year, Month, Hour, whether they are out of state, and if they got a ticket. The glimpse information is below (18 by 9773). Not all these variables might be classfied as explanatory variables, but they are key indicators to predict whether or not a ticket was issued.

```{r}
# subset to last year
last.year<-2017
stops.full$AutoYear<-as.numeric(stops.full$Year)
stops.full$Year<-as.numeric(substr(stops.full$Date.Of.Stop,7,10))
stops.last<-subset(stops.full,Year==last.year)
# delete the really big data set ... don't need to tie up the memory
rm(stops.full)

# Create Month and Hour variables
stops.last$Month<-as.numeric(substr(stops.last$Date.Of.Stop,1,2))
stops.last$Hour<-as.numeric(substr(stops.last$Time.Of.Stop,1,2))

# clean up dataset
stops.last$AutoState<-stops.last$State
stops.last$Out.of.State<-(stops.last$AutoState!="MD")

stops.last$Color<-as.character(stops.last$Color)
stops.last$Color[stops.last$Color %in% c("CAMOUFLAGE","CHROME","COPPER","CREAM","MULTICOLOR","N/A","PINK")]<-"OTHER"
stops.last$Color<-factor(stops.last$Color)

# other filters
stops.last<-subset(stops.last,Color != "N/A")
stops.last<-subset(stops.last,Color != "")
stops.last<-subset(stops.last,Gender != "U")
stops.last<-subset(stops.last,HAZMAT == "No")
stops.last<-subset(stops.last,AutoYear > 1990 & AutoYear < last.year+2)

# convert character variables to factors
stops.last$SubAgency<-factor(stops.last$SubAgency)
stops.last$Accident<-factor(stops.last$Accident)
stops.last$Belts<-factor(stops.last$Belts)
stops.last$Personal.Injury<-factor(stops.last$Personal.Injury)
stops.last$Property.Damage<-factor(stops.last$Property.Damage)
stops.last$Commercial.License<-factor(stops.last$Commercial.License)
stops.last$Commercial.Vehicle<-factor(stops.last$Commercial.Vehicle)
stops.last$Alcohol<-factor(stops.last$Alcohol)
stops.last$Work.Zone<-factor(stops.last$Work.Zone)
stops.last$Contributed.To.Accident<-factor(stops.last$Contributed.To.Accident)
stops.last$Race<-factor(stops.last$Race)
stops.last$Gender<-factor(stops.last$Gender)
stops.last$Out.of.State<-factor(stops.last$Out.of.State)

# Create dataset for Speeding
#  example: EXCEEDING MAXIMUM SPEED: 49 MPH IN A POSTED 40 MPH ZONE
speed.last1<-subset(stops.last,substr(Description,1,23)=="EXCEEDING MAXIMUM SPEED")
# difference between cited speed and posted speed limit
speed.last1$speed<-as.numeric(substr(speed.last1$Description,26,27))-as.numeric(substr(speed.last1$Description,45,46))
speed.last1<-subset(speed.last1,!is.na(speed))
#  example: EXCEEDING POSTED MAXIMUM SPEED LIMIT: 39 MPH IN A POSTED 30 MPH ZONE
speed.last2<-subset(stops.last,substr(Description,1,30)=="EXCEEDING POSTED MAXIMUM SPEED")
# difference between cited speed and posted speed limit
speed.last2$speed<-as.numeric(substr(speed.last2$Description,39,40))-as.numeric(substr(speed.last2$Description,58,59))
speed.last2<-subset(speed.last2,!is.na(speed))
# combine and subset to columns of interest
speed.last<-rbind(speed.last1,speed.last2)
speed.last<-speed.last[,c(4,9:12,14,16:18,24,28:30,36:38,40,41)]

glimpse(speed.last)
```

Here are the summary statistics and the histogram of the speed data. Speed in this set indicates the mph over the speed limit when stopped for a violation. The shape of this data is skewed to the right with a mean of about 15 mph over the speed limit and standard deviation of 7.36 mph. There are, of course, other considerations we have to make. If you've ever had a parking citation, you would know! Usually officers mark down the speed at the time they stop you, so the information we get in the end may not be a clear reflection of a traffic violation. Realistically, we could probably go faster and get it marked down. 

```{r}
c(summary(speed.last$speed),"sd"=sd(speed.last$speed))
hist(speed.last$speed)
```

Now, let's create the train and test datasets. As a rule of thumb, the train dataset usually consists of about 70~80% of the complete dataset, and the train is comprised of the rest. The datasets are created by randomly sampling directly out of the dataset. For this step, make sure to set a random seed so you can make sure to get opposite values of train and test. After confirming that the train and test datasets are similar, we're good to fit the model!
```{r}


# Check length of dataset 
length(speed.last$SubAgency)*.80 # 80% = close to 8000 observations


# Create Train and Test
# Create a SRS without replacement of n.train=8000
set.seed(12) # Choose whatever seed!.
train.rows <- sample(length(speed.last$SubAgency),8000)
speed.train <- speed.last[train.rows,]
speed.test <- speed.last[-train.rows,]


# confirm train and test are smimilar
rbind("Train" = summary(speed.train$speed), "Test" = summary(speed.test$speed))

```
So, the package you need to download is the randomForest. This has a variety of help functions for classification and regression with rf. I haven't played around with it too much, but some interesting and helpful functions include the MDSplot and the na.roughfix. These may be nice to visualize and fix missing values of your dataset.

Now the randomForest model is quite simple. for the x value, we input the training dataset EXCLUDING the response variable (speed for this case) you are measuring. The y value will include the response variable (which is the same as column 18). The same is done for the xtest and ytext values for the functions (make sure you put the test dataset with x/ytest and x and y for train. Pretty obvious, but easy to mix up). Another specification to make is the replace = TRUE which makes sure to bootstrap your samples, so you can create as many trees as there are bootstrap possibilities (x^n possibile, distinct outcomes). 

The other fine tuning of parameters can get a little messy depending on how specific you want information. Since a lot of websites do not specify what parameters to use, I will do my best to make a general outline of what you can use to play around with.

ntree: The number of trees you want to create. 
  The more trees you make, the better more in-depth your analysis will be, yet there is always a point where marginal benefit meets the marginal cost. Computationally, the expected variance will decrease as the the sample size increases sd/sqrt(n) so collecting a large sample is obviously good, yet the more times the training value is being used to predict, there is a prediction error associated with each training sample. Hence, the larger the the sample, the more errors we are stacking up. So, depending on the size of the dataset we should be careful. You can take a look at the OOB (out of bag) error rate for the optimum tree size and predictors sampled here. As a general rule of thumb, you can use about 50-130 trees so you can have a relative accuracy and not let your CPU die because of too much data.
https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/

mtry: The number of predictors sampled for each tree.
  Each tree has a different number of predictor variables randomly used. In this model, we have 50 trees that use 5 bootstrapped predictors for each tree. The determination for how many to use can also be viewed through the OOB error rate. If you weren't too meticulous, just use about a third of the number of explanatory variables. Keeping a relatively small set of explanatory varaibles is ideal as it allows the trees to make more interactions with specific variables.
  
nodesize: The minimum number of data you can have in each node.
  Since we are working with recursive partitions, our data tends to be heiarchial. We have branches of n size data that branch out with other data of various sizes. The smaller the number, the larger the trees. One way to figure out the optimum is by taking nodesizes from 10,20,30,40,50 and comparing the RMSE (Root Mean Square Error) of the data, which we will do afterwards. IF the RMSE of the train dataset is similar to the test, we can be assured that we have a good model. If they don't seem to work even then, just keep increasing the nodesize. Larger datasets will usually require larger nodesizes.
  
  
```{r}
# install.packages("randomForest")
library("randomForest")

names(speed.train) # to check which one is speed. Take out speed no. 18
out.speed <- randomForest(x=speed.train[,-18], y=speed.train$speed,
                          xtest=speed.test[,-18], ytest=speed.test$speed,
                          replace=TRUE, # Bootstrap SRS with replacement
                          keep.forest=TRUE, ntree=50, mtry=5, nodesize=30)
out.speed
```
Looking out the output from out.speed, we can see that the Train MSE (variance, or mean of squared residuals) is 49.08 and the Test MSE is 50.25 assuring us that their root values are also similar. 
Of course, an RMSE of about 7 is rather large considering the speeds range from 1 all the way to 61. This means that 95% of the data will fall about +- 14 mph from the average speed of getting a ticket. We can see that the % variance explained (R^2) is small, which gives an indication of poorer prediction performance. For the sake of this example I will not tune the parameters further, but you get the gist.

If there is one thing we need to be careful of, is overfitting. That is when the tree is finding a distinctive or peculiar feature of the observed data and is too close to be true. The only way to solve this is by tuning the parameters above. Usually, these problems can be resolved when more complex correlations can be observed between the data, so one way you may be able to resolve this problem is by increasing the number of trees and decreasing mtry and node size. This shouldn't be too big of a concern with Random Forests, but this usually depends on what your dataset looks like.

Now let's see how many times each explanatory variable is used in the tree. The more it is used, the more important it is, the less it is used, the less important. An important point to note is that importance is not the same as significance. Just because the variable was the number one predictor, does not mean those differences are statistically significant. This can be done by a simple test such as an ANOVA.

```{r}
# Model Interpretability(how often were explanatory variables used in the forest?)
head(importance(out.speed))
varImpPlot(out.speed)
```

The **_importance()_** function and the plot highlights a significant feature of the random forest. As we can see, accident and commercial vehicle measures were not used at all. Intuitively we know how accidents are a result, rather than an explanation for bad driving. I left it in to show that the rf is extremely intelligent: It seeks variables that have the best fit and ommit unnecessary variables (unless it overfits of course.. Then you'll have the rf making ludacris connections). 


Now that we have our model, let's make a prediction! First, we will grab an observation in the dataset that is very similar to my situation and *change it*. This is NOT a reflection of myself, but just creating a hypothetical person. Afterwards, we can use the **_predict()_** function to figure out:

How fast can you drive around Washington DC before you will get a ticket?

```{r}
# make a prediction at a new observation
# note: 
new.obs<-speed.last[25,]
new.obs$AutoYear<-2017
new.obs$Month<-8
new.obs$Hour<-18
glimpse(new.obs)

predict(out.speed,newdata=new.obs)

hist(speed.last$speed)
abline(v=predict(out.speed,newdata=new.obs), col="red")

```

Wouldn't that be neat? I can speed at about 15 miles an hour before I get caught in Montgomery County, MD! 
Of course, I do have to keep in mind how the RMSE is about 7 miles an hour, which will make the confidence interval rather large. Regardless, we have ourselves a rf prediction! 

As you can see, Random Forest is quite the attractive model. While this example was using a quantatative predictor variable, we can also move to categorical models as well. If we wanted, we can also answer the question of: If pulled over in Washington DC will you get a ticket or a warning? Depending on the characteristics of the individual, of course. If you want to see the code for that, send me a message and I'll contact you directly. 

Overall, we have learned that the RF model takes random subsets of all data at each node and finds a variable that optimizes at each split. This analysis technique can predict both quantatative and categorical data. Another plus is how missing values do not affect the analysis overall! All we need to do is to use different explanatory variables to predict an explanation for either type of data. Of course, there are assumptions to be made, 1. We cannot predict anything outside of the information we have. 2.  The correlation between the explanatory and predictor are roughly linear. 3. homoscedasticity of errors.


Fortunately, The Montgomery County Traffic Violations data is probably one of the best datasets I have used since it doesn't seem like they break too many of the assumptions. In the future, maybe I should use more trees, it would most likely reduce the chance of stumbling across a classifier that doesn’t perform well because of the relationship between the train and test data. Maybe I'll give it a try when I have time.

I hope you enjoyed my code and analysis! (:

