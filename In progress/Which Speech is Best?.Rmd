---
title: "Predict which speech is best?"
output: html_notebook
---

This may seem a little cynical..

![](https://i.imgur.com/GfGA2u6.png)


Ted talks are so popular, "TED.com currently have over 2,500+ talks, with addition every single day. tedx - YouTube channel, there are currently a little over 100,000 videos with 9.3M subscribers" (TED.com). It's surreal how demanded these "motivational" speakers are. 

Our goal is to create our own speaker session by discovering which topics are the most popular. Of course, that is definitely not enough to create a business model, however we this will be some important research. 

[Here](https://raw.githubusercontent.com/tykiww/projectpage/master/datasets/Ted/ted.csv) is our dataset of TED talks. It's a rather large file (7MB), so be carful with it running computations. 

Here are the libraries to use for today.

```{r}
library("tidyverse")
library("glmnet")
library("plotly")
```

To figure out what type of analysis to run, let's take a `glimpse` into our data (if you're not interested in the data and just into how to run the model with the parameters, keep scrolling down).

```{r}
ted <- read.csv('ted.csv', stringsAsFactors=FALSE) %>% as.tibble
glimpse(ted)
```

    ## Observations: 2,550
    ## Variables: 17
    ## $ comments           <int> 4553, 265, 124, 200, 593, 672, 919, 46, 852, 900, 79, 55, 71, 242, 99, 325, 305, 8...
    ## $ description        <chr> "Sir Ken Robinson makes an entertaining and profoundly moving case for creating an...
    ## $ duration           <int> 1164, 977, 1286, 1116, 1190, 1305, 992, 1198, 1485, 1262, 1414, 1538, 1550, 527, 1...
    ## $ event              <chr> "TED2006", "TED2006", "TED2006", "TED2006", "TED2006", "TED2006", "TED2006", "TED2...
    ## $ film_date          <int> 1140825600, 1140825600, 1140739200, 1140912000, 1140566400, 1138838400, 1140739200...
    ## $ languages          <int> 60, 43, 26, 35, 48, 36, 31, 19, 32, 31, 27, 20, 24, 27, 25, 31, 32, 27, 22, 32, 27...
    ## $ main_speaker       <chr> "Ken Robinson", "Al Gore", "David Pogue", "Majora Carter", "Hans Rosling", "Tony R...
    ## $ name               <chr> "Ken Robinson: Do schools kill creativity?", "Al Gore: Averting the climate crisis...
    ## $ num_speaker        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
    ## $ published_date     <int> 1151367060, 1151367060, 1151367060, 1151367060, 1151440680, 1151440680, 1152490260...
    ## $ ratings            <chr> "[{'id': 7, 'name': 'Funny', 'count': 19645}, {'id': 1, 'name': 'Beautiful', 'coun...
    ## $ related_talks      <chr> "[{'id': 865, 'hero': 'https://pe.tedcdn.com/images/ted/172559_800x600.jpg', 'spea...
    ## $ speaker_occupation <chr> "Author/educator", "Climate advocate", "Technology columnist", "Activist for envir...
    ## $ tags               <chr> "['children', 'creativity', 'culture', 'dance', 'education', 'parenting', 'teachin...
    ## $ title              <chr> "Do schools kill creativity?", "Averting the climate crisis", "Simplicity sells", ...
    ## $ url                <chr> "https://www.ted.com/talks/ken_robinson_says_schools_kill_creativity\n", "https://...
    ## $ views              <int> 47227110, 3200520, 1636292, 1697550, 12005869, 20685401, 3769987, 967741, 2567958,...

Not bad on size. For us to figure out which explanatory variables will predict well, we will probably need to use some robust nonlinear regression. However, with this <i>p</i> of columns, we will definitely need to do some feature selection. In the past, we ran a [stepwise regresssion](https://tykiww.github.io/2017-12-05-Stepwise-CVD/). However, this time we can use a continuous method: LASSO.

Lasso is an acronym for Least Absolute Shrinkage and Selection Operator. Mathematically, Lasso is a regularization model that penalizes the number of features in a model in order to only keep the most important features. 

brilliant. LASSO limits the absolute sum of coefficients in a regression model by shrinking the high coefficients (these will usually overfit anyways) and small coefficients to zero. In a sense, it is shrunk to reduce multicollinearity since these are more likely to be low signal to noise, increasing predictive power. It uses 2 parameters alpha and lambda. 

    - Alpha is the parameter which decides whether to minimize the Reduced Sum of squares or coefficient sum of squares. It usually takes values rom 0 to 1 where 0 is our ordinary least squares estimate, 0.5 is our ridge regression, and 1 is our elastic net. We will need to set this value on our own.
    - Lambda is the shrinkage parameter. It is otherwise known as the penalty coefficient that increases with the number of our variables. To select, we search for the minimum lambda value after cross validating.

We won't get deep into the math. If you are interested in learning, send me a message or check out [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/). 

**But how come a lasso over stepwise?** In all honesty, stepwise is computationally heavy and annoying. With Lasso, you don't need to utilize an arbitrary percentage # feature to keep, since some of those may not be informative. Furthermore, you lose significant interpretability in your model as it focuses on prediction. The motivation leading to stepwise regression is that you ahve a lot of potential predictors, but not enough data to estimate their coefficients in a meaningful way. Since the model is taken out of the constraint of ordinary least squares, we don't need to worry about collinearity.

<hr>

Alright, we are good to go.

The way we analyze the model will be to use predictor variables of comments, durations, number of speakers, tag data, and ratings data against our total viewership. I've manipulated the data so we will be able to run it straight away. If you're curious about how I cleaned up the [data](https://raw.githubusercontent.com/tykiww/projectpage/master/datasets/Ted/lasso_ready.csv), take a look at my code [here](https://raw.githubusercontent.com/tykiww/projectpage/master/Uncataloged-R-Projects/tedclean.R).

```{r}
# load the clean dataset.
paths <- "https://raw.githubusercontent.com/tykiww/projectpage/master/datasets/Ted/lasso_ready.csv"
lasso_part <- read_csv(paths)
```

Lasso in R only takes matrix X's and a vector for the predictor.



```{r}
x <- model.matrix(views ~ ., data = lasso_part)[,-1]
y <- lasso_part$views
```



```{r}
# Using cross validation to determine optimal lambda
k <- 3

possible_lambdas <- seq(0, 200, 1)

lambda_rpmses <- c()

for (l in possible_lambdas) {
  print(l/length(possible_lambdas)*100)
  rpmses <- rep(0, k)
  for (cv in 1:k) {
    train_rows <- sample.int(n = nrow(lasso_part), size = floor(.8 * nrow(lasso_part)), replace = F)
    lasso_model <- glmnet(x[train_rows,], y[train_rows], lambda = l)
    
    preds <- predict(lasso_model, s = l, newx = x[-train_rows,])
    
    rpmses[cv] <- sqrt(mean((preds - y[-train_rows])^2))
  }
  lambda_rpmses <- c(lambda_rpmses, mean(rpmses))
}

```



```{r}
# Looking at potential alpha values and associated rpmse
data.frame("lambda" = possible_lambdas, "rpmse" = lambda_rpmses) %>%
  ggplot(aes(lambda, rpmse)) +
  geom_point()

# Getting the alpha that minimizes rpmse
best_lambda <- possible_lambdas[which.min(lambda_rpmses)]

# Fitting the model as required by part c
lasso_model <- glmnet(x[train_rows,], y[train_rows], lambda = best_lambda)

##### PARTS E & F #####
sort(coef(lasso_model), decreasing = T)
coef(lasso_model)[1,1]
```


# as for interpretation..
LASSO seeks to estimate the same coefficients as OLS maximum likelihood does. The model is the same, and the interpretation remains the same. The numerical values from LASSO will normally differ from those from OLS maximum likelihood, but if a sensible amount of penalization has been applied, the LASSO estimates will be closer to the true values than the OLS maximum likelihood estimates.
